---
title: 'Weekly Exercises #4'
author: "Gabriel Reynolds"
output: 
  html_document:
    keep_md: TRUE
    toc: TRUE
    toc_float: TRUE
    df_print: paged
    code_download: true
    code_folding: hide 
    theme: yeti
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error=FALSE, message=FALSE, warning=FALSE)
```

```{r libraries}
library(tidyverse)     # for data cleaning and plotting
library(lubridate)     # for date manipulation
library(openintro)     # for the abbr2state() function
library(palmerpenguins)# for Palmer penguin data
library(maps)          # for map data
library(ggmap)         # for mapping points on maps
library(gplots)        # for col2hex() function
library(RColorBrewer)  # for color palettes
library(sf)            # for working with spatial data
library(leaflet)       # for highly customizable mapping
library(carData)       # for Minneapolis police stops data
library(ggthemes)      # for more themes (including theme_map())
theme_set(theme_minimal())
```

```{r data}
# Starbucks locations
Starbucks <- read_csv("https://www.macalester.edu/~ajohns24/Data/Starbucks.csv")

starbucks_us_by_state <- Starbucks %>% 
  filter(Country == "US") %>% 
  count(`State/Province`) %>% 
  mutate(state_name = str_to_lower(abbr2state(`State/Province`))) 

# Lisa's favorite St. Paul places - example for you to create your own data
favorite_stp_by_lisa <- tibble(
  place = c("Home", "Macalester College", "Adams Spanish Immersion", 
            "Spirit Gymnastics", "Bama & Bapa", "Now Bikes",
            "Dance Spectrum", "Pizza Luce", "Brunson's"),
  long = c(-93.1405743, -93.1712321, -93.1451796, 
           -93.1650563, -93.1542883, -93.1696608, 
           -93.1393172, -93.1524256, -93.0753863),
  lat = c(44.950576, 44.9378965, 44.9237914,
          44.9654609, 44.9295072, 44.9436813, 
          44.9399922, 44.9468848, 44.9700727)
  )

#COVID-19 data from the New York Times
covid19 <- read_csv("https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv")

```

## Put your homework on GitHub!

If you were not able to get set up on GitHub last week, go [here](https://github.com/llendway/github_for_collaboration/blob/master/github_for_collaboration.md) and get set up first. Then, do the following (if you get stuck on a step, don't worry, I will help! You can always get started on the homework and we can figure out the GitHub piece later):

* Create a repository on GitHub, giving it a nice name so you know it is for the 4th weekly exercise assignment (follow the instructions in the document/video).  
* Copy the repo name so you can clone it to your computer. In R Studio, go to file --> New project --> Version control --> Git and follow the instructions from the document/video.  
* Download the code from this document and save it in the repository folder/project on your computer.  
* In R Studio, you should then see the .Rmd file in the upper right corner in the Git tab (along with the .Rproj file and probably .gitignore).  
* Check all the boxes of the files in the Git tab under Stage and choose commit.  
* In the commit window, write a commit message, something like "Initial upload" would be appropriate, and commit the files.  
* Either click the green up arrow in the commit window or close the commit window and click the green up arrow in the Git tab to push your changes to GitHub.  
* Refresh your GitHub page (online) and make sure the new documents have been pushed out.  
* Back in R Studio, knit the .Rmd file. When you do that, you should have two (as long as you didn't make any changes to the .Rmd file, in which case you might have three) files show up in the Git tab - an .html file and an .md file. The .md file is something we haven't seen before and is here because I included `keep_md: TRUE` in the YAML heading. The .md file is a markdown (NOT R Markdown) file that is an interim step to creating the html file. They are displayed fairly nicely in GitHub, so we want to keep it and look at it there. Click the boxes next to these two files, commit changes (remember to include a commit message), and push them (green up arrow).  
* As you work through your homework, save and commit often, push changes occasionally (maybe after you feel finished with an exercise?), and go check to see what the .md file looks like on GitHub.  
* If you have issues, let me know! This is new to many of you and may not be intuitive at first. But, I promise, you'll get the hang of it! 


## Instructions

* Put your name at the top of the document. 

* **For ALL graphs, you should include appropriate labels.** 

* Feel free to change the default theme, which I currently have set to `theme_minimal()`. 

* Use good coding practice. Read the short sections on good code with [pipes](https://style.tidyverse.org/pipes.html) and [ggplot2](https://style.tidyverse.org/ggplot2.html). **This is part of your grade!**

* When you are finished with ALL the exercises, uncomment the options at the top so your document looks nicer. Don't do it before then, or else you might miss some important warnings and messages.


## Warm-up exercises from tutorial

These exercises will reiterate what you learned in the "Mapping data with R" tutorial. If you haven't gone through the tutorial yet, you should do that first.

### Starbucks locations (`ggmap`)

  1. Add the `Starbucks` locations to a world map. Add an aesthetic to the world map that sets the color of the points according to the ownership type. What, if anything, can you deduce from this visualization?  
  
```{r}
world <- get_stamenmap(
    bbox = c(left = -180, bottom = -57, right = 179, top = 82.1), 
    maptype = "terrain",
    zoom = 2)

Starbucks_Clean <- Starbucks %>% 
  rename(ownership_type = `Ownership Type`)
  
ggmap(world) + 
  geom_point(data = Starbucks_Clean, 
             aes(x = Longitude, 
                 y = Latitude, 
                 color = ownership_type), 
             alpha = .4, 
             size = 1) +
  labs(title = "Worldwide Starbucks Locations and Ownership Type", 
       color = "Ownership Type", 
       x = "",
       y = "")
             

```

**Observations:** It appears that most Starbucks are either company owned or licensed. As far as I can tell, there are very few franchise Starbucks. North America and Europe are particularly dominated by licensed and company owned stores. Interestingly, stores that are joint ventures are mostly found in Eastern Europe, S. Asia, and E. Asia. There are very few Starbucks in S.America and Africa-- two areas that produce a lot of coffee beans. It's difficult to deduce much beyond these spatial distribution patterns. 

  2. Construct a new map of Starbucks locations in the Twin Cities metro area (approximately the 5 county metro area).  
  
```{r}
twin_cities <- get_stamenmap(
    bbox = c(left = -93.5, bottom = 44.8, right = -92.8, top = 45.2), 
    maptype = "terrain",
    zoom = 11)

Starbucks_TC <- Starbucks_Clean %>% 
  filter(Country == "US",
         `State/Province` == "MN")

ggmap(twin_cities) + 
  geom_point(data = Starbucks_TC, 
             aes(x = Longitude, 
                 y = Latitude)) +
  labs(title = "Starbucks in the Twin Cities")
```


  3. In the Twin Cities plot, play with the zoom number. What does it do?  (just describe what it does - don't actually include more than one map).  
  
**Discussion:** The zoom number changes the level of detail in the map. The higher the number, the more zoomed in the map is, and the more detail it provides. As the number decreases, the map zooms out, showing a greater area but with less detail. 

  4. Try a couple different map types (see `get_stamenmap()` in help and look at `maptype`). Include a map with one of the other map types. 
  
```{r}
twin_cities2 <- get_stamenmap(
    bbox = c(left = -93.5, bottom = 44.8, right = -92.8, top = 45.2), 
    maptype = "toner-2010",
    zoom = 11)

ggmap(twin_cities2) + 
  geom_point(data = Starbucks_TC, 
             aes(x = Longitude, 
                 y = Latitude), 
             color = "red") +
  labs(title = "Starbucks in the Twin Cities", 
       x = "", 
       y = "")
```

  5. Add a point to the map that indicates Macalester College and label it appropriately. There are many ways you can do think, but I think it's easiest with the `annotate()` function (see `ggplot2` cheatsheet).
  
```{r}
ggmap(twin_cities2) + 
  geom_point(data = Starbucks_TC, 
             aes(x = Longitude, 
                 y = Latitude), 
             size = .5, 
             color = "dark green") +
  annotate("point",
           x = -93.1691, 
           y = 44.9379, 
           label = "Macalester College", 
           color = "orange", 
           size = 3) +
   annotate("text",
           x = -93.1691, 
           y = 44.92, 
           label = "Macalester College", 
           color = "orange", 
           size = 3) +
  labs(title = "Starbucks in TC + Macalester College", 
       x = "", 
       y = "")
```


### Choropleth maps with Starbucks data (`geom_map()`)

The example I showed in the tutorial did not account for population of each state in the map. In the code below, a new variable is created, `starbucks_per_10000`, that gives the number of Starbucks per 10,000 people. It is in the `starbucks_with_2018_pop_est` dataset.

```{r}
census_pop_est_2018 <- read_csv("https://www.dropbox.com/s/6txwv3b4ng7pepe/us_census_2018_state_pop_est.csv?dl=1") %>% 
  separate(state, into = c("dot","state"), extra = "merge") %>% 
  select(-dot) %>% 
  mutate(state = str_to_lower(state))

starbucks_with_2018_pop_est <-
  starbucks_us_by_state %>% 
  left_join(census_pop_est_2018,
            by = c("state_name" = "state")) %>% 
  mutate(starbucks_per_10000 = (n/est_pop_2018)*10000)
```

  6. **`dplyr` review**: Look through the code above and describe what each line of code does.
  
**Answer:** The first line of code reads in the csv file and creates a data frame with its information called 'census_pop_est_2018.' Following that line of code, the next one use the separate function to turn the 'state' column into two separate columns that separate the state name from the period in front of it. Next, the select function is used to select all variables in the data frame except the period, which in this case means its just the state name. Then, you use the mutate function to essentially change the state variable using the str_to_lower function to make all the states' names be in lower case. 

The code of chunk that follows creates a new data frame called 'starbucks_with_2018_pop_est' by piping into the starbucks_us_by_state data frame. From it, this new data frame uses the left_join function to append the starbucks_us_by_state data frame with the variables from the census_pop_est_2018 data frame. These data sets are joined by identifying the unique id that is shared between the two of the data sets, in this case 'state name' and 'state', and appending the information based on this shared variable. The new data set is then given another variable with the mutate function called 'starbucks_per_10000.' This new variable calculates the number of Starbucks per 10000 people by dividing the number of stores in each state by the population of each state, and then multiplying that by 10000.  

  7. Create a choropleth map that shows the number of Starbucks per 10,000 people on a map of the US. Use a new fill color, add points for all Starbucks in the US (except Hawaii and Alaska), add an informative title for the plot, and include a caption that says who created the plot (you!). Make a conclusion about what you observe.

```{r}
US_Map <- map_data("state")

Starbucks_US <- Starbucks %>% 
  filter(Country == "US")

Starbucks_US_Points <- starbucks_with_2018_pop_est %>% 
  left_join(Starbucks_US, 
            by = c("State/Province")) %>% 
  filter(!state_name %in% c("alaska", "hawaii"))

Starbucks_US_Points %>% 
  rename(region = state_name) %>% 
  ggplot() +
  geom_map(map = US_Map, 
           aes(map_id = region, 
               fill = starbucks_per_10000)) + 
  scale_fill_gradient(low = "#27251F", high = "#00704A") +
  expand_limits(x = US_Map$long, y = US_Map$lat) + 
  theme_map() +
  geom_point(data = Starbucks_US_Points, 
             aes(x = Longitude, 
                 y = Latitude), 
             color = "grey",
             size = 1, 
             alpha = .5) +
  theme(legend.background = element_blank(), 
        legend.position = 'right') +
  labs(title = "Starbucks per 10,000 People", 
       caption = "Gabriel Reynolds", 
       fill = "Starbucks per 10,000 People")

```

**Discussion:** Interestingly, there is a higher number of Starbucks per 10,000 people in the western part of the US than anywhere else. Visually, it appears that there are many more stores in the East, but considering the population distribution, apparently there are more Starbucks per people in the West, which I suppose makes sense because of Starbucks' founding in Seattle. Colorado also stands out as a state with a very high number of stores per 10,000 people, which I wasn't expecting. 

### A few of your favorite things (`leaflet`)

  8. In this exercise, you are going to create a single map of some of your favorite places! The end result will be one map that satisfies the criteria below. 

  * Create a data set using the `tibble()` function that has 10-15 rows of your favorite places. The columns will be the name of the location, the latitude, the longitude, and a column that indicates if it is in your top 3 favorite locations or not. For an example of how to use `tibble()`, look at the `favorite_stp_by_lisa` I created in the data R code chunk at the beginning.  

  * Create a `leaflet` map that uses circles to indicate your favorite places. Label them with the name of the place. Choose the base map you like best. Color your 3 favorite places differently than the ones that are not in your top 3 (HINT: `colorFactor()`). Add a legend that explains what the colors mean.  
  
  * Connect all your locations together with a line in a meaningful way (you may need to order them differently in the original data).  
  
  * If there are other variables you want to add that could enhance your plot, do that now.  
  
```{r}
favorite_places <- tibble(
  name = c("minnehaha_falls", "saint_anthony_falls", "como_park", "art_institute", "grand_marais", "door_county", "duluth", "aster_cafe", "bell_museum", "pike_island"), 
  long = c(-93.209, -93.255848, -93.154268, -93.274030, -90.337863, -87.194083, -92.097201, -93.255249, -93.187747, -93.166682), 
  lat = c(44.915001, 44.982000, 44.982330, 44.958326, 47.751534, 44.985930, 46.788668, 44.984456, 44.991583, 44.891313),
  top_3 = c(TRUE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE)
)

pal <- colorFactor(palette=c("blue", "orange"), 
                     domain = favorite_places$top_3)

leaflet(favorite_places) %>% 
  addProviderTiles(providers$Stamen.Toner) %>% 
  addCircles(label = ~name,
             color = ~pal(top_3)) %>% 
  addLegend("topleft", pal = pal, values = ~top_3, bins=1, title = "Top 3 Favorite Places") %>%
  addPolylines(lng = c(-93.209, -93.274030, -93.255848, -93.255249, -92.097201, -90.337863, -87.194083, -93.166682, -93.154268, -93.187747), 
               lat = c(44.915001,44.958326, 44.982000, 44.984456, 46.788668, 47.751534, 44.985930, 44.891313, 44.982330, 44.99158),
               color="purple", weight = 3)

```

  
## Revisiting old datasets

This section will revisit some datasets we have used previously and bring in a mapping component. 

### Bicycle-Use Patterns

The data come from Washington, DC and cover the last quarter of 2014.

Two data tables are available:

- `Trips` contains records of individual rentals
- `Stations` gives the locations of the bike rental stations

Here is the code to read in the data. We do this a little differently than usualy, which is why it is included here rather than at the top of this file. To avoid repeatedly re-reading the files, start the data import chunk with `{r cache = TRUE}` rather than the usual `{r}`. This code reads in the large dataset right away.

```{r cache=TRUE}
data_site <- 
  "https://www.macalester.edu/~dshuman1/data/112/2014-Q4-Trips-History-Data.rds" 
Trips <- readRDS(gzcon(url(data_site)))
Stations<-read_csv("http://www.macalester.edu/~dshuman1/data/112/DC-Stations.csv")
```

  9. Use the latitude and longitude variables in `Stations` to make a visualization of the total number of departures from each station in the `Trips` data. Use either color or size to show the variation in number of departures. This time, plot the points on top of a map. Use any of the mapping tools you'd like.
  
```{r}
Trips2 <- Trips %>% 
  group_by(sstation) %>% 
  summarize(n_departures = n()) %>% 
  arrange(desc(n_departures)) %>% 
  left_join(Stations, 
            by = c("sstation"="name")) 


pal2 <- colorBin(palette="YlGnBu",
                 domain = Trips2$n_departures,
                 bins=6)

leaflet(Trips2) %>% 
  addProviderTiles(providers$Stamen.Toner) %>% 
  addCircles(label = ~sstation, 
             color = ~pal2(n_departures), 
             opacity = 1) %>% 
  addLegend("topleft", values = ~n_departures, pal=pal2, bins=6, title="Number of Station Departures")

```
  
  10. Only 14.4% of the trips in our data are carried out by casual users. Create a plot that shows which area(s) have stations with a much higher percentage of departures by casual users. What patterns do you notice? Also plot this on top of a map. I think it will be more clear what the patterns are.
  
```{r}
CasualTrips <- Trips %>% 
  group_by(sstation) %>%
  mutate(casual = client == "Casual", 
         registered = client == "Registered") %>% 
  group_by(sstation) %>% 
  summarize(prop_casual = sum(casual)/(sum(registered) + sum(casual))) %>% 
  arrange(desc(prop_casual)) %>% 
  left_join(Stations, 
            by = c("sstation"="name")) 

pal3 <- colorBin(palette = "YlGnBu", 
                 domain = CasualTrips$prop_casual)

leaflet(CasualTrips) %>% 
  addProviderTiles(providers$Stamen.Toner) %>% 
  addCircles(label = ~sstation, 
             color = ~pal3(prop_casual), 
             opacity = 1) %>% 
  addLegend("bottomleft", pal = pal3, values = ~prop_casual, bins = 6, title = "Proportion of Casual Clients")
```

**Observations:** With the map, it's clearer to identify which stations have higher proportions of causal clients and why. It appears that the stations with the highest proportion of casual clients are around key tourist attractions in D.C. For instance, there are stations around the main Mall, the Washington Monument, the MLK Memorial, the Lincoln Memorial, and the Capitol Building all with notably higher proportions of casual clients. This makes sense as tourists will be more likely to visit these areas and choose to commute with bikes.  
  
### COVID-19 data

The following exercises will use the COVID-19 data from the NYT.

  11. Create a map that colors the states by the most recent cumulative number of COVID-19 cases (remember, these data report cumulative numbers so you don't need to compute that). Describe what you see. What is the problem with this map?

```{r}
StatesMap <- map_data("state")

covid19 %>% 
  mutate(region = tolower(state)) %>% 
  group_by(region) %>% 
  mutate(cum_cases = max(cases)) %>% 
  ggplot() +
  geom_map(map = StatesMap,
           aes(map_id = region,
               fill = cum_cases)) +
  expand_limits(x = StatesMap$long, y = StatesMap$lat) + 
  theme_map() + 
  labs(title = "COVID-19 Cases by State", 
       x = "", 
       y = "", 
       fill = "Total Cumulative Cases by State") +
  theme(legend.position = "right")
```

**Discussion:** This map shows the number of cumulative COVID-19 cases by state. From the map, we can see that California, Texas, New York, Florida, and Illinois are amonng the states with the most cases in the US. The issue with this map is that it's looking at raw counts of cases per state, rather than a per capita measure (a rate). Only looking at raw data will make it such that the most populous states will always appear to be the most in magnitude. IT wouldn't be fair to compare raw counts of covid cases between California and Wyoming, the population sizes are drastically different. It would be more appropriate to look at a standardized measure or rate of COVID cases instead. 

  12. Now add the population of each state to the dataset and color the states by most recent cumulative cases/10,000 people. See the code for doing this with the Starbucks data. You will need to make some modifications. 
  
```{r}
covid19_with_pop_est <- covid19 %>%
  mutate(state = tolower(state)) %>% 
  left_join(census_pop_est_2018, 
            by = c("state")) %>% 
  group_by(state) %>% 
  mutate(cum_cases = max(cases), 
            cases_per_10000 = (cum_cases/est_pop_2018) * 10000)

covid19_with_pop_est %>% 
  ggplot() +
  geom_map(map = StatesMap,
           aes(map_id = state,
               fill = cases_per_10000)) +
  expand_limits(x = StatesMap$long, y = StatesMap$lat) + 
  theme_map() + 
  labs(title = "COVID-19 Cases per 10,000 People by State", 
       x = "", 
       y = "", 
       fill = "Total Cumulative Cases per 10000 People by State") +
  theme(legend.position = "bottom")
```

  
  13. **CHALLENGE** Choose 4 dates spread over the time period of the data and create the same map as in exercise 12 for each of the dates. Display the four graphs together using faceting. What do you notice?

```{r}
covid_4day_sample <- covid19 %>% 
  mutate(state = tolower(state)) %>% 
  left_join(census_pop_est_2018, 
            by = c("state")) %>% 
  group_by(date, state) %>% 
  mutate(cum_cases = max(cases), 
            cases_per_10000 = (cum_cases/est_pop_2018) * 10000)

covid_4day_sample %>% 
  filter(date == "2021-01-01"|
         date == "2020-10-01"|
         date == "2020-07-01"|
         date == "2020-04-01") %>% 
  ggplot() +
  geom_map(map = StatesMap,
           aes(map_id = state,
               fill = cases_per_10000)) +
  expand_limits(x = StatesMap$long, y = StatesMap$lat) + 
  theme_map() +
  labs(title = "COVID-19 Cases per 10,000 People by State", 
       x = "", 
       y = "", 
       fill = "Total Cumulative Cases per 10000 People by State") +
  theme(legend.position = "bottom")+
  facet_wrap(vars(date))
  
```

**Discussion:** You can see from the maps that Covid cases per 10,000 have drastically increased since the beginning of the pandemic. Because the scale is fixed for each of the maps, its bound to the upper limits of the most recent case counts, which are much much higher than to start. This causes the initial maps in the earlier months of the pandemic to appear as if the cases were very low, though this is only a result of the high cases numbers today. As such, we're unable to see the gradation in the severity of cases across states until about October, which still doesn't provide a ton of clarity around which states are doing worse/better than others. You can see throughout these maps, however, that the Pacific Northwest has consistently low cases throughout this time period sample. I'm surprised to see how high the cases per 10,000 has gotten for the Dakotas, it appears they are among the highest case counts in the country. 

## Minneapolis police stops

These exercises use the datasets `MplsStops` and `MplsDemo` from the `carData` library. Search for them in Help to find out more information.

  14. Use the `MplsStops` dataset to find out how many stops there were for each neighborhood and the proportion of stops that were for a suspicious vehicle or person. Sort the results from most to least number of stops. Save this as a dataset called `mpls_suspicious` and display the table.  
  
```{r}
mpls_suspicious <- MplsStops %>% 
  mutate(sus = problem == "suspicious") %>% 
  group_by(neighborhood) %>% 
  summarize(n_stops = n(), 
            sus_stops = sum(sus), 
            prop_sus = sus_stops/n_stops) %>% 
  arrange(desc(n_stops))

head(mpls_suspicious)
```

  
  15. Use a `leaflet` map and the `MplsStops` dataset to display each of the stops on a map as a small point. Color the points differently depending on whether they were for suspicious vehicle/person or a traffic stop (the `problem` variable). HINTS: use `addCircleMarkers`, set `stroke = FAlSE`, use `colorFactor()` to create a palette.  

```{r}
pal4 <- colorFactor(palette = c("blue", "yellow"), 
                    domain = MplsStops$problem)

leaflet(MplsStops) %>% 
  addProviderTiles(providers$Stamen.Toner) %>% 
  addCircles(lng = ~long,
             lat = ~lat,
             radius = 3,
             color = ~pal4(problem),
             stroke=FALSE, 
             label= ~problem) %>% 
    addLegend("topleft", pal = pal4, values = ~problem, bins=2, title = "Stop Type")

```


  16. Save the folder from moodle called Minneapolis_Neighborhoods into your project/repository folder for this assignment. Make sure the folder is called Minneapolis_Neighborhoods. Use the code below to read in the data and make sure to **delete the `eval=FALSE`**. Although it looks like it only links to the .sph file, you need the entire folder of files to create the `mpls_nbhd` data set. These data contain information about the geometries of the Minneapolis neighborhoods. Using the `mpls_nbhd` dataset as the base file, join the `mpls_suspicious` and `MplsDemo` datasets to it by neighborhood (careful, they are named different things in the different files). Call this new dataset `mpls_all`.

```{r}
mpls_nbhd <- st_read("Minneapolis_Neighborhoods/Minneapolis_Neighborhoods.shp", quiet = TRUE)
```

```{r}
mpls_suspicious_full <- mpls_suspicious %>% 
  left_join(MplsStops,
            by="neighborhood")

mpls_nbhd_copy <- mpls_nbhd %>% 
  left_join(mpls_suspicious_full,
            by=c("BDNAME"="neighborhood"))

mpls_all <- mpls_nbhd_copy %>% 
  left_join(MplsDemo, 
            by=c("BDNAME"="neighborhood"))
```


  17. Use `leaflet` to create a map from the `mpls_all` data  that colors the neighborhoods by `prop_suspicious`. Display the neighborhood name as you scroll over it. Describe what you observe in the map.
  
```{r}
  pal5 <- colorBin(palette="Greens",
                 domain = mpls_all$prop_sus,
                 bins=6)

leaflet(mpls_all) %>% 
  addProviderTiles(providers$Stamen.Toner) %>% 
  addCircles(lng = ~long,
             lat = ~lat,
             radius = 3,
             stroke=FALSE, 
             label = ~BDNAME,
             color = ~pal5(prop_sus),
             opacity=1) %>% 
  addLegend("topleft", values = ~prop_sus, pal=pal5, bins=6, title="Proportion of Suspicious Police Stops")
  
```

**Observations:** The Neighborhoods that appear to have the highest proportion of police stops for people deemed suspicious are clustered around the downtown area and the neighborhoods in south Minneapolis that are along 35W such as East Phillips, Central, Powderhorn Park, Seward, Longfelllow, and Corcoran. There are few stops for 'suspicious' people in Northeast Minneapolis and around other parts near the U of M campus. Same with chain of lakes area. From personally living in the Twin Cities all my life, I know these trends to roughly equate to the affluence of these neighborhoods. The neighborhoods with wealthier people tend to have lower proportions of stops for people who are 'suspicious.' 
  
  18. Use `leaflet` to create a map of your own choosing. Come up with a question you want to try to answer and use the map to help answer that question. Describe what your map shows. 

**Question:** Are police stops in certain neighborhoods more likely to result in a vehicle search than other neighborhoods? 

```{r}
VehicleSearched <- MplsStops %>%
  filter(!is.na(vehicleSearch)) %>% 
  mutate(search_yes = vehicleSearch == "YES") %>% 
  group_by(neighborhood) %>% 
  summarize(n_stops = n(), 
            v_search_yes = sum(search_yes == TRUE), 
            prop_search_yes = v_search_yes/n_stops) %>% 
  arrange(desc(prop_search_yes))

VehicleSearched_full <- VehicleSearched %>% 
  left_join(MplsStops,
            by="neighborhood")

VehicleSearched_nbhd <- mpls_nbhd %>% 
  left_join(VehicleSearched_full,
            by=c("BDNAME"="neighborhood"))

VehicleSearchedAll <- VehicleSearched_nbhd %>% 
  left_join(MplsDemo, 
            by=c("BDNAME"="neighborhood"))

pal6 <- colorBin(palette = "Greens", 
                    domain = VehicleSearchedAll$prop_search_yes, 
                    bins = 6)

leaflet(VehicleSearchedAll) %>% 
  addProviderTiles(providers$Stamen.Toner) %>% 
  addCircles(lng = ~long,
             lat = ~lat,
             radius = 3,
             stroke=FALSE, 
             label = ~BDNAME,
             color = ~pal6(prop_search_yes),
             opacity=1) %>% 
  addLegend("topright", values = ~prop_search_yes, pal=pal6, bins=1, title="Proportion of Stops that Led to a Vehicle Search")
  
```

**Discussion:** My map illustrates the proportion of stops by the police that result in a vehicle search, categorized by neighborhood in Minneapolis. Areas on the map that are darker indicate a higher proportion of total stops that result in a vehicle search. From the map, we can see a clear spatial pattern. Vehicle searches happen at a higher rate in North Minneapolis and the Phillips Neighborhood than the rest of Minneapolis. Without more context and data analysis, it might be erroneous of me to make any causal inferences. I do suspect, however, that this is indicative of a racial imbalance in the proportion of vehicle searches that occur as North Minneapolis and the Phillips neighborhood have a higher percentage of POC living in these areas. We can definitely see that there is disparity in terms of the rate at which vehicle searches occur across neighborhoods. I would be curious to see how different the map might be and how different the rates might be if I had calculated for any search, whether of a person or of a vehicle. 
  
## GitHub link

  19. Below, provide a link to your GitHub page with this set of Weekly Exercises. Specifically, if the name of the file is 04_exercises.Rmd, provide a link to the 04_exercises.md file, which is the one that will be most readable on GitHub.

**Link:** https://github.com/greynolds1121/Reynolds_Week_4_HW/blob/main/04_exercises.md

**DID YOU REMEMBER TO UNCOMMENT THE OPTIONS AT THE TOP?**
